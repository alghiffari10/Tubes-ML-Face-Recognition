{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megha1906/TransferLearning-using-VGG16/blob/master/Transfer_Learning_using_vgg16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "bzoJOGXqD9TK",
        "outputId": "b267a26e-df66-49d5-aab3-b7f7f007ca26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-27 10:24:39.426371: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-27 10:24:39.508698: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-27 10:24:39.508945: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-27 10:24:39.512882: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-27 10:24:39.528612: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-27 10:24:39.530199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-27 10:24:40.833934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 512)               0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              525312    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16815426 (64.15 MB)\n",
            "Trainable params: 2100738 (8.01 MB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from keras.applications import vgg16\n",
        "\n",
        "# MobileNet was designed to work on 224 x 224 pixel input images sizes\n",
        "img_rows, img_cols = 224, 224 \n",
        "\n",
        "# Re-loads the MobileNet model without the top or FC layers\n",
        "vgg = vgg16.VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))\n",
        "\n",
        "# Here we freeze the last 4 layers \n",
        "# Layers are set to trainable as True by default\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(vgg.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)\n",
        "    \n",
        " \n",
        "def lw(bottom_model, num_classes):\n",
        "    \"\"\"creates the top or head of the model that will be \n",
        "    placed ontop of the bottom layers\"\"\"\n",
        "\n",
        "    top_model = bottom_model.output\n",
        "    top_model = GlobalAveragePooling2D()(top_model)\n",
        "    top_model = Dense(1024,activation='relu')(top_model)\n",
        "    top_model = Dense(1024,activation='relu')(top_model)\n",
        "    top_model = Dense(512,activation='relu')(top_model)\n",
        "    top_model = Dense(num_classes,activation='softmax')(top_model)\n",
        "    return top_model \n",
        "    \n",
        "    \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "# Set our class number to 3 (Young, Middle, Old)\n",
        "num_classes = 2\n",
        "\n",
        "FC_Head = lw(vgg, num_classes)\n",
        "\n",
        "model = Model(inputs = vgg.input, outputs = FC_Head)\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "JwLWKQpnEjy3",
        "outputId": "9c48d2b0-8449-4816-d1b8-1058b9911b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 268 images belonging to 2 classes.\n",
            "Found 250 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_dir = 'photos/photos/train'\n",
        "validation_data_dir = 'photos/photos/validate'\n",
        "\n",
        "# Let's use some data augmentaiton \n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=45,\n",
        "      width_shift_range=0.3,\n",
        "      height_shift_range=0.3,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "# set our batch size (typically on most mid tier systems we'll use 16-32)\n",
        "batch_size = 32\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "colab_type": "code",
        "id": "n2jJ0uVoE0tA",
        "outputId": "ce8dc733-9b4a-4e0b-b699-700788d7ef0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n",
            "/tmp/ipykernel_16021/84430091.py:33: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-27 10:24:44.329025: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 411041792 exceeds 10% of free system memory.\n",
            "2023-11-27 10:24:44.502517: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 411041792 exceeds 10% of free system memory.\n",
            "2023-11-27 10:24:45.244728: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 205520896 exceeds 10% of free system memory.\n",
            "2023-11-27 10:24:45.458716: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 205520896 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/6 [====>.........................] - ETA: 26s - loss: 0.7123 - accuracy: 0.2812"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-27 10:24:48.564417: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 411041792 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - ETA: 0s - loss: 1.4953 - accuracy: 0.5156\n",
            "Epoch 1: val_loss improved from inf to 0.56968, saving model to face recognisation.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/raihan/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 39s 7s/step - loss: 1.4953 - accuracy: 0.5156 - val_loss: 0.5697 - val_accuracy: 0.6354\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5765 - accuracy: 0.7500\n",
            "Epoch 2: val_loss did not improve from 0.56968\n",
            "6/6 [==============================] - 35s 7s/step - loss: 0.5765 - accuracy: 0.7500 - val_loss: 0.7994 - val_accuracy: 0.3854\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3568 - accuracy: 0.8140\n",
            "Epoch 3: val_loss improved from 0.56968 to 0.50427, saving model to face recognisation.h5\n",
            "6/6 [==============================] - 67s 13s/step - loss: 0.3568 - accuracy: 0.8140 - val_loss: 0.5043 - val_accuracy: 0.7812\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.8837\n",
            "Epoch 4: val_loss improved from 0.50427 to 0.48686, saving model to face recognisation.h5\n",
            "6/6 [==============================] - 67s 12s/step - loss: 0.2930 - accuracy: 0.8837 - val_loss: 0.4869 - val_accuracy: 0.8542\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3557 - accuracy: 0.8281\n",
            "Epoch 5: val_loss improved from 0.48686 to 0.40733, saving model to face recognisation.h5\n",
            "6/6 [==============================] - 69s 12s/step - loss: 0.3557 - accuracy: 0.8281 - val_loss: 0.4073 - val_accuracy: 0.8854\n"
          ]
        }
      ],
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "                     \n",
        "checkpoint = ModelCheckpoint(\"face recognisation.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 3,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "callbacks = [earlystop, checkpoint]\n",
        "\n",
        "# We use a very small learning rate \n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = RMSprop(lr = 0.001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# Enter the number of training and validation samples here\n",
        "nb_train_samples = 108\n",
        "nb_validation_samples = 52\n",
        "\n",
        "# We only train 5 EPOCHS \n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sRY44SUYGWwA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class - aamir \n",
            "Predicted class: aamir \n",
            "Class -  raihan\n",
            "Predicted class:  raihan \n",
            "Class - aamir \n",
            "Predicted class: aamir \n",
            "Class -  raihan\n",
            "Predicted class: aamir \n",
            "Class - aamir \n",
            "Predicted class: aamir \n",
            "Class -  raihan\n",
            "Predicted class:  raihan \n",
            "Class -  raihan\n",
            "Predicted class:  raihan \n",
            "Class -  raihan\n",
            "Predicted class:  raihan \n",
            "Class - aamir \n",
            "Predicted class: aamir \n",
            "Class - aamir \n",
            "Predicted class: aamir \n"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "classifier = load_model('face recognisation.h5')\n",
        "\n",
        "monkey_breeds_dict = {\"[0]\": \" raihan \", \"[1]\": \"aamir \"}\n",
        "monkey_breeds_dict_n = {\"n0\": \" raihan\", \"n1\": \"aamir \"}\n",
        "\n",
        "def draw_test(name, pred, im):\n",
        "    monkey = monkey_breeds_dict[str(pred)]\n",
        "    BLACK = [0, 0, 0]\n",
        "    expanded_image = cv2.copyMakeBorder(im, 80, 0, 0, 100, cv2.BORDER_CONSTANT, value=BLACK)\n",
        "    cv2.putText(expanded_image, monkey, (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "    cv2.imshow(name, expanded_image)\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "\n",
        "\n",
        "def print_prediction(pred):\n",
        "    monkey = monkey_breeds_dict[str(pred)]\n",
        "    print(\"Predicted class: {}\".format(monkey))\n",
        "\n",
        "def getRandomImage(path):\n",
        "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
        "    random_directory = np.random.randint(0, len(folders))\n",
        "    path_class = folders[random_directory]\n",
        "    print(\"Class - \" + monkey_breeds_dict_n[str(path_class)])\n",
        "    file_path = path + path_class\n",
        "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
        "    random_file_index = np.random.randint(0, len(file_names))\n",
        "    image_name = file_names[random_file_index]\n",
        "    return cv2.imread(file_path + \"/\" + image_name)\n",
        "\n",
        "for i in range(0, 10):\n",
        "    input_im = getRandomImage(\"photos/photos/validate/\")\n",
        "    input_original = input_im.copy()\n",
        "    input_original = cv2.resize(input_original, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    input_im = cv2.resize(input_im, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "    input_im = input_im / 255.\n",
        "    input_im = input_im.reshape(1, 224, 224, 3)\n",
        "    \n",
        "    # Draw prediction\n",
        "    # draw_test(\"Prediction\", res, input_original)\n",
        "\n",
        "    # Get Prediction\n",
        "    res = np.argmax(classifier.predict(input_im, 1, verbose=0), axis=1)\n",
        "\n",
        "    # Print the predicted class\n",
        "    print_prediction(res)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM4c7zcLOgm1LMxQOqbZBUC",
      "include_colab_link": true,
      "name": "Transfer Learning using vgg16.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
